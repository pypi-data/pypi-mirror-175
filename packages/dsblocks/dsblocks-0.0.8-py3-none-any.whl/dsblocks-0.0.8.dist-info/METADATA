Metadata-Version: 2.1
Name: dsblocks
Version: 0.0.8
Summary: DS Blocks
Home-page: https://github.com/Jaume-JCI/ds-blocks
Author: Jaume Amores
Author-email: jamorej@jci.com
License: Apache Software License 2.0
Keywords: nbdev jupyter notebook python
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Natural Language :: English
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: License :: OSI Approved :: Apache Software License
Requires-Python: >=3.7,<=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pandas
Requires-Dist: scikit-learn
Requires-Dist: pyarrow
Requires-Dist: numpy
Requires-Dist: ipython
Requires-Dist: matplotlib
Requires-Dist: graphviz
Requires-Dist: optuna
Requires-Dist: sh
Requires-Dist: ipynbname
Requires-Dist: tensorflow
Requires-Dist: rich
Provides-Extra: dev
Requires-Dist: pytest ; extra == 'dev'
Requires-Dist: pytest-cov ; extra == 'dev'
Requires-Dist: ipdb ; extra == 'dev'
Requires-Dist: jupyter ; extra == 'dev'
Requires-Dist: ipykernel ; extra == 'dev'
Requires-Dist: hpsearch ; extra == 'dev'

DS Blocks
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

`DS Blocks` makes it easy to write highly modular and compact data
science pipelines. It is based on a generalization of the well-known
scikit-learn pipeline design, enriching and extending it in multiple
ways. By doing so, `DS Blocks` makes it possible to express the ML
solution in terms of independent building blocks that can be easily
moved around and reused to create different solutions. At the same time,
`DS Blocks` makes it possible to write concise code by automatically
taking care of common steps that are needed when building a data science
pipeline, resulting in a significant reduction of boiler-plate code.

`DS Blocks` also provides a number of features that facilitate working
with notebooks, such as: - Integration with
[nbdev](https://nbdev.fast.ai/) and extension of its functionalities. to
use pytest as a test engine. `nbdev` is a powerful framework that
stream-lines development on notebooks using best software practices.
`DS Blocks` extends `nbdev` by making it possible to convert notebooks
into a test suite for external engines such as `pytest`. It also allows
convenient freezing and unfreezing notebook test cells to avoid
recomputing the tests when every time we need to restart and re-run the
notebook. - `DS Blocks` provides several magic functions that facilitate
reproducibility. It also provides convenient decorators for converting
functions into pipeline components and reducing boiler-plate. - In
addition to a powerful pipeline design (see below), `DS Blocks` provides
out-of-the-box components frequently used in Data Science, such as for
cross-validation and model-selection in general, building ensembles,
working with time-series, and more.

## Features

The following is a selection of some of the benefits provided by using
`DS Blocks` pipelines:

- Automatize common steps that are usually present in ML code, including
  caching / loading of intermediate results across the entire pipeline,
  logging, profiling, conversion of data to appropriate format, and
  more.

- Easy debugging of the entire pipeline, both during the current run as
  well as post-mortem. Facilitates investigation of issues occurred
  during past runs.

- Make it possible to easily show statistics and other types of
  information about the output of each component in the pipeline, print
  a summary of the pipeline, plot a diagram of the components, and show
  the dimensionality of the output provided by each component.

- Extend scikit-learn pipelines in several ways, including: i) make it
  possible to use any data type in the communication between components.
  This is done through data conversion layers that facilitate reusing
  the components across different pipelines, regardless of the data
  format used by rest of the components. In particular, two important
  data types enabled in `DS Blocks` are DataFrames and dictionaries.
  Using pandas DataFrame is suitable for many data science problems such
  as time-series analysis, making it easy to visualize different periods
  of time, subsets of variables and categories (i.e., normal vs
  anomaly). While standard scikit-learn components accept DataFrame as
  input data type, the output is always provided as a numpy array,
  making it necessary to manually convert the output back to the
  original DataFrame format every time, with the corresponding
  proliferation of boiler-plate code. `DS Blocks` enables a consistent
  use of DataFrames across the whole pipeline: when the input is a
  DataFrame, the output will be a DataFrame as well, and when the input
  is a numpy array the output is a numpy array.

- Enable the use of sampling components that not only change the
  variables (or columns) but also change the number of observations (or
  rows), by either under-sampling or over-sampling. This is not
  supported by standard scikit-learn components.

## Comparison against other frameworks

`DS Blocks` provides functionalities that are also present in frameworks
such as [Metaflow](https://metaflow.org/),
[Kedro](https://kedro.readthedocs.io/), \[Ploomber\]
(https://ploomber.io/) and others. In this section we briefly comment on
the differences against these three frameworks, which are among the most
popular ones. The main difference wrt to these frameworks is the use of
a compact design loosely similar to scikit-learn’s estimator API, which
allows to concisely express any ML solution with pipeline components
that are familiar in the community. Another important difference is
that, while our design allows to build any kind of Directed Acyclic
Graph (DAG), we do not need to express the edges of such graph
explicitly, reducing the corresponding boiler-plate. Apart from those
differences, we comment here on more specific differences wrt each
framework:

- The main difference wrt frameworks such as `Kedro`, is that we use a
  pure-code approach, avoding the need of writing separate config files
  that govern the behaviour of the pipeline.
- The main difference wrt to `Metaflow`, is that we can to a large
  extent keep the original code without by either wrapping it with
  simple decorators and classes, or just constructing our pipeline as a
  sequence of the original functions and classes. While `Metaflow`
  allows to create flows of original functions, it uses a more verbose
  approach for achieving this, as shown in the examples.
- The main difference with `Ploomber`, `Luigi`, and other frameworks is
  that our pipelines are constructed programmatically with pure python,
  not by gluing together the inputs and outputs of applications that are
  run separately.

## Installation

DS Blocks is pip installable:

``` bash
pip install dsblocks
```

## Example usage

### Baseline problem

As a first baseline example, we start by taking
[Optuna](https://optuna.org/#code_examples)’s quadratic problem: find
the value of $X$ that minimizes:

$$(X-2)^2$$

This is implemented using three simple functions:

``` python
from dsblocks import Sequential, Component
import numpy as np
```

``` python
def subtract2 (X): 
    return X-2
def square (X): 
    return X*X

pipeline = Sequential (subtract2, square, np.argmin)
```

``` python
X = np.arange (5)
idx_min = pipeline (X)
print (f'index of X with optimal solution: {idx_min}, value: {X[idx_min]}')
```

    index of X with optimal solution: 2, value: f2

Sequential pipes the results from one function into the next, the final
one being `np.argmin`. In this toy example each function performs a
simple calculation, but in general they perform time-consuming
processes.

Many times, the first step of pipeline is to get the data from an
external source or storage. We now augment the pipeline to include this
step and also include persistence and logging:

``` python
def get_data ():
    return np.arange (5)

pipeline = Sequential (get_data, subtract2, square, np.argmin,
                       verbose=2, path_results='square_problem')

pipeline()
```

    applying pipeline (on whole data)
    loading from /home/jcidatascience/jaume/workspace/remote/ds-blocks/square_problem/whole/pipeline_result.pk
    loaded pre-computed result

    2

Now we can easily load the results of intermediate steps:

``` python
pipeline.subtract2.load_result ()
```

    loading from /home/jcidatascience/jaume/workspace/remote/ds-blocks/square_problem/whole/subtract2_result.pk

    array([-2, -1,  0,  1,  2])

``` python
pipeline.square.load_result()
```

    loading from /home/jcidatascience/jaume/workspace/remote/ds-blocks/square_problem/whole/square_result.pk

    array([4, 1, 0, 1, 4])

We can simulate the case where there was an interrumption in the
execution and we need to resume it:

``` python
rm square_problem/whole/subtract2_result.pk
```

``` python
rm square_problem/whole/square_result.pk
```

``` python
rm square_problem/whole/pipeline_result.pk
```

``` python
pipeline ()
```

    applying pipeline (on whole data)
    applying get_data (on whole data)
    loading from /home/jcidatascience/jaume/workspace/remote/ds-blocks/square_problem/whole/get_data_result.pk
    loaded pre-computed result
    applying subtract2 (on whole data)
    saving to /home/jcidatascience/jaume/workspace/remote/ds-blocks/square_problem/whole/subtract2_result.pk
    applying square (on whole data)
    saving to /home/jcidatascience/jaume/workspace/remote/ds-blocks/square_problem/whole/square_result.pk
    applying argmin (on whole data)
    loading from /home/jcidatascience/jaume/workspace/remote/ds-blocks/square_problem/whole/argmin_result.pk
    loaded pre-computed result
    saving to /home/jcidatascience/jaume/workspace/remote/ds-blocks/square_problem/whole/pipeline_result.pk

    2

We can see that the first intermediate results, from `get_data` and
`subtract2` are loaded, while the remaining steps, whose results we
removed from disk, are computed and their results saved.

### Example using data converters

Let us modify the above problem as follows: we want to find the
hyper-parameter `c` that minimizes the following regression problem:

$$
(X+c)^T (X+c) = Y,
$$

given a simple 1D dataset:

$$
X = (0, 1, 2)^T \\
Y = (4, 9, 16)^T
$$

where the ground-truth $y_i$ is always $y_i = (x_i+2) ^ 2$, and
therefore the optimal solution is $c=2$.

``` python
from dsblocks.core.data_conversion import DataConverter
from sklearn.metrics import mean_squared_error
```

``` python
def get_data ():
    X = np.array ([0, 1, 2])
    Y = np.array ([4, 9, 16])
    return X, Y

def add_c (X, c):
    return X+c

def square (X):
    return X*X

# wrong pipeline, see below
pipeline = Sequential (get_data, add_c, square, mean_squared_error)
```

There are two issues with the above pipeline:

- The first function `get_data ()` provides a result with two variables:
  `X` and `Y`. The subsequent components `add_c` and `square` do not
  expect to receive `Y`, but the last function, `mean_squared_error`
  needs it. We address this by using *data converters*, which in our
  case drop the Y variable in all the cases except in the last step
  where it is needed.

- The function `add_c` has an additional argument `c` that is not
  provided by the previous function. We address this by passing `c` as a
  parameter of the first component. This will allow us later to run N
  pipelines in parallel, one for each candidate value of `c`, so that we
  can find a good value.

Before illustrating how those items are typically implemented with
`DS Blocks`, let us first see a more standard solution: using wrappers
for data conversion, and partial functions for indicating the value of
`c` in the pipeline. The use of wrappers is suitable for instance if we
have external functions that are used in our pipeline.

``` python
from functools import partial
```

``` python
def ignore_labels (func):
    def wrapper (X, Y):
        # 1. "data conversion" before calling function: Y is dropped, and only X is passed
        result = func (X)
        # 2. "data conversion" after calling the function: Y is attached to the result
        return result, Y
    return wrapper

c = 0 # pipeline parametrized with c=0
pipeline = Sequential (get_data, 
                       ignore_labels (partial (add_c, c=c)), 
                       ignore_labels (square), 
                       mean_squared_error)
error = pipeline () 
print (f'the error obtained with c={c} is {error}')
```

    the error obtained with c=0 is 74.66666666666667

While the previous approach works fine in the previous example, our
pipelines also accept classes of objects with several methods, not only
simple functions. In particular, our pipelines accept estimators that
have methods similar to `fit`, `predict` and `transform`. For such case,
it is more convenient to use
[`DataConverter`](https://Jaume-JCI.github.io/ds-blocks/core/data_conversion.html#dataconverter)
objects as illustrated in the code below. A similar thing happens
regarding the use of `partial`. Since we might have multiple methods
whose parameters we migth want to indicate at construction time, we need
something more generic, and concise, than partial. The next code
illustrates these ideas in `DS Blocks`.

``` python
from dsblocks.core.data_conversion import DataConverter
```

``` python
class IgnoreLabels (DataConverter):
    def __init__ (self, **kwargs):
        super ().__init__ (**kwargs)
    def convert_before_applying (self, X, Y, **kwargs):
        self.Y = Y
        return X
    def convert_after_applying (self, result, **kwargs):
        return result, self.Y
        
pipeline = Sequential (get_data,
                       Component(add_c, c=c, data_converter=IgnoreLabels),
                       Component(square, data_converter=IgnoreLabels), 
                       mean_squared_error)

error = pipeline () 
print (f'the error obtained with c={c} is {error}')
```

    the error obtained with c=0 is 74.66666666666667

### Using [`Parallel`](https://Jaume-JCI.github.io/ds-blocks/core/compose.html#parallel)

We can estimate the error obtained by multiple values of the parameter
`c`, using a
[`Parallel`](https://Jaume-JCI.github.io/ds-blocks/core/compose.html#parallel)
object. This object is a pipeline similar to `Sequential` but where the
outputs are not piped linearly from one step to the next. By default,
the same initial input is fed to all the components that compose the
[`Parallel`](https://Jaume-JCI.github.io/ds-blocks/core/compose.html#parallel)
object, and the output from all of them is gathered in a tuple. Both
things can be, however, configured though callbacks. Let us see how it
works in our case.

``` python
from dsblocks import Parallel
```

``` python
pipelines = (Sequential (get_data, 
                         Component(add_c, c=c, data_converter=IgnoreLabels),
                         Component(square, data_converter=IgnoreLabels), 
                         mean_squared_error)
              for c in range(0,5))

parallel = Parallel (*pipelines)

result = parallel ()
```

As we can see, our
[`Parallel`](https://Jaume-JCI.github.io/ds-blocks/core/compose.html#parallel)
object is composed of 5 pipeline components, each pipeline receiving a
different value of parameter `c`. The
[`Parallel`](https://Jaume-JCI.github.io/ds-blocks/core/compose.html#parallel)
object then runs those pipelines and gathers their result in a tuple.

In general, the
[`Parallel`](https://Jaume-JCI.github.io/ds-blocks/core/compose.html#parallel)
object can be constructed by passing any collection of components, and
this collection can be heterogeneous. While in the current case, we have
constructed multiple copies of the same `Sequential` object, we can as
well have a single copy that receives different values of `c` each time,
by using the
[`ParallelInstances`](https://Jaume-JCI.github.io/ds-blocks/core/compose.html#parallelinstances)
class. However, its use is currently a bit more elaborate and we leave
this topic for an advanced tutorial. Let us see now the error as a
function of `c`:

``` python
import matplotlib.pyplot as plt
```

``` python
plt.plot (result, 'b.-')
```

![](index_files/figure-commonmark/cell-21-output-1.png)

### Fitting models and sub-classing [`Component`](https://Jaume-JCI.github.io/ds-blocks/core/components.html#component)

``` python
from dsblocks import Component, Sequential
from dsblocks.core.data_conversion import DataConverter
from sklearn.metrics import mean_squared_error
```

``` python
def get_data ():
    X = np.array ([0, 1, 2])
    Y = np.array ([4, 9, 16])
    return X, Y

class BruteForceModel (Component):
    def __init__ (self, c_values=range(5), **kwargs):
        super().__init__ (**kwargs)
    
    def _apply (self, X):
        return (X+self.c)**2

    def _fit (self, X, Y):
        error = np.empty ((len(self.c_values),))
        for i, c in enumerate(self.c_values):
            self.c = c
            Y_hat = self._apply (X)
            error[i] = mean_squared_error (Y, Y_hat)
        self.c = self.c_values[np.argmin (error)]

pipeline = Sequential (get_data, 
                       BruteForceModel, 
                       Component(mean_squared_error, data_converter='NoConverter'))
pipeline.fit_apply ()
```

    0.0

``` python
class IgnoreLabels (DataConverter):
    def __init__ (self, **kwargs):
        super ().__init__ (**kwargs)
    
    # transform
    def convert_before_applying (self, X, Y, **kwargs):
        self.Y = Y
        return X
    def convert_after_applying (self, result, **kwargs):
        return result, self.Y
    
    # fit
    def convert_before_fitting (self, X, Y, **kwargs):
        self.Y = Y
        return X, Y

pipeline = Sequential (Component (get_data, data_converter='NoConverter'),
                       BruteForceModel (data_converter=IgnoreLabels),
                       Component(mean_squared_error, data_converter='NoConverter'))

pipeline.fit_apply ()

print (f'value of c: {pipeline.brute_force_model.c}')
```

    value of c: 2

``` python
class BruteForceModel ():
    def __init__ (self, c_values=range(5), **kwargs):
        self.c_values = c_values
    
    def transform (self, X):
        return (X+self.c)**2

    def fit (self, X, Y):
        error = np.empty ((len(self.c_values),))
        for i, c in enumerate(self.c_values):
            self.c = c
            Y_hat = self.transform (X)
            error[i] = mean_squared_error (Y, Y_hat)
        self.c = self.c_values[np.argmin (error)]
        return self

pipeline = Sequential (Component (get_data, data_converter='NoConverter'),
                       Component (BruteForceModel(), data_converter=IgnoreLabels),
                       Component (mean_squared_error, data_converter='NoConverter'))
pipeline.fit_apply ()
```

    0.0

### Using MultiSplit objects

``` python
from sklearn.model_selection import train_test_split
from dsblocks.core.compose import MultiSplitDict
```

``` python
def get_data (n=225, noise=1.0):
    X = np.arange (n) 
    Y = (X+2)**2 + np.random.randn (n) * noise
    return X, Y

def generate_split (X, Y, proportion_training=0.8):
    n_samples_train=int(len(X)*proportion_training)
    n_samples_test=len(X)-n_samples_train
    X_train, X_test, Y_train, Y_test = train_test_split(
        X, Y, train_size=n_samples_train, test_size=n_samples_test, shuffle=False
    )
    data = dict (training=(X_train, Y_train),
                 test=(X_test, Y_test))
    return data

brute_force_model = BruteForceModel(c_values=[1.8, 1.9, 2.0, 2.1, 2.2])
pipeline = Sequential (get_data, 
                       Component(generate_split, data_converter='NoConverter'), 
                       MultiSplitDict (Component(brute_force_model, data_converter=IgnoreLabels)), 
                       MultiSplitDict(Component(mean_squared_error, data_converter='NoConverter')))
pipeline.fit_apply()
```

    {'training': 1.0385118754467242, 'test': 0.7858863403210958}

### Using experiment tracking

``` python
from dsblocks.core.compose import SequentialWithTracking

tracking_pipeline = SequentialWithTracking (pipeline)
```

    could not pickle object: Can't pickle <class 'dsblocks.core.utils.get_ds_experiment_manager.<locals>.DSExperimentManager'>: it's not found as dsblocks.core.utils.get_ds_experiment_manager.<locals>.DSExperimentManager
    could not pickle object: Can't pickle <class 'dsblocks.core.utils.get_ds_experiment_manager.<locals>.DSExperimentManager'>: it's not found as dsblocks.core.utils.get_ds_experiment_manager.<locals>.DSExperimentManager

We can run the pipeline with different parameters and have the results
tracked

``` python
error = tracking_pipeline.fit_apply (n=5, noise=1000)
print (f'error: {error}')
print (f'fitted parameter: {tracking_pipeline.cls.BruteForceModel.estimator.c}')
```

    error: {'training': 577610.2579625886, 'test': 8241488.206360425}
    fitted parameter: 1.8

``` python
data=joblib.load ('.dsblocks/main/experiments/00000/0/whole/generate_split_result.pk')
print ('training X: ', data['training'][0], '\ntraining Y: ', data['training'][1])
```

    training X:  [0 1 2 3] 
    training Y:  [-1461.89345869   -37.84917166   180.99375497   389.09645245]

``` python
error = tracking_pipeline.fit_apply (n=1000, noise=1000)
print (f'error: {error}')
print (f'fitted parameter: {tracking_pipeline.cls.BruteForceModel.estimator.c}')
```

    error: {'training': 943850.4502534339, 'test': 986840.6863515133}
    fitted parameter: 1.9

``` python
error = tracking_pipeline.fit_apply (n=10000, noise=1000)
print (f'error: {error}')
print (f'fitted parameter: {tracking_pipeline.cls.BruteForceModel.estimator.c}')
```

    error: {'training': 1015048.3910494718, 'test': 992893.6028738584}
    fitted parameter: 2.0

``` python
em = tracking_pipeline.get_experiment_manager ()
```

``` python
df = em.get_experiment_data ()
df [['parameters','scores']]
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="3" halign="left">parameters</th>
      <th colspan="2" halign="left">scores</th>
    </tr>
    <tr>
      <th></th>
      <th>function</th>
      <th>n</th>
      <th>noise</th>
      <th>test</th>
      <th>training</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th>0</th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>fit_apply</td>
      <td>5</td>
      <td>1000</td>
      <td>8241488.20636</td>
      <td>577610.257963</td>
    </tr>
    <tr>
      <th>1</th>
      <td>fit_apply</td>
      <td>1000</td>
      <td>1000</td>
      <td>986840.686352</td>
      <td>943850.450253</td>
    </tr>
    <tr>
      <th>2</th>
      <td>fit_apply</td>
      <td>10000</td>
      <td>1000</td>
      <td>992893.602874</td>
      <td>1015048.391049</td>
    </tr>
  </tbody>
</table>
</div>

``` python
em.grid_search (
    parameters_multiple_values=dict (n=[int(1e5), int(1e6), int(1e7)],
                                     noise=[10.0,100.0,1000.0])
)
```

``` python
df = tracker.get_experiment_data()
```

``` python
df_to_analyze = df[df[('parameters','noise')]==100.0]
parameter_to_analyze = df_to_analyze[('parameters','n')].values
scores_training = df_to_analyze[('scores','training')].values
scores_test = df_to_analyze[('scores','test')].values

plt.plot (parameter_to_analyze, scores_training, 'b.-')
plt.plot (parameter_to_analyze, scores_test, 'm.-')
plt.xlabel ('size of training set')
plt.ylabel ('error')
plt.legend (['training', 'test'])
```

    <matplotlib.legend.Legend at 0x7f7c689c6050>

![](index_files/figure-commonmark/cell-37-output-2.png)

### Integration with Optuna

``` python
def parameter_sampler (trial):
    noise = trial.suggest_uniform('noise', 1000, 10000)
    n = trial.suggest_categorical('n', [100, 250, 500])

    parameters = dict(noise=noise,
                      n=n)

    return parameters

em.key_score = 'test'
em.hp_optimization (parameter_sampler=parameter_sampler)
```

    [I 2022-11-07 15:57:01,416] Using an existing study with name 'hp_study' instead of creating a new one.
    [I 2022-11-07 15:57:01,860] Trial 1 finished with value: 40098705.3785892 and parameters: {'noise': 5939.321535345923, 'n': 100}. Best is trial 1 with value: 40098705.3785892.
    [I 2022-11-07 15:57:02,274] Trial 2 finished with value: 29027633.822648745 and parameters: {'noise': 4812.8931940501425, 'n': 500}. Best is trial 1 with value: 40098705.3785892.
    [I 2022-11-07 15:57:02,692] Trial 3 finished with value: 112301196.54020809 and parameters: {'noise': 9672.964844509264, 'n': 250}. Best is trial 3 with value: 112301196.54020809.
    [I 2022-11-07 15:57:03,141] Trial 4 finished with value: 26854230.185282387 and parameters: {'noise': 6112.401049845391, 'n': 100}. Best is trial 3 with value: 112301196.54020809.
    [I 2022-11-07 15:57:03,593] Trial 5 finished with value: 1322441.1874790885 and parameters: {'noise': 1181.9655769629314, 'n': 500}. Best is trial 3 with value: 112301196.54020809.
    [I 2022-11-07 15:57:04,041] Trial 6 finished with value: 81736165.02785024 and parameters: {'noise': 9807.565080094875, 'n': 100}. Best is trial 3 with value: 112301196.54020809.
    [I 2022-11-07 15:57:04,506] Trial 7 finished with value: 3852312.850922469 and parameters: {'noise': 2064.469832820399, 'n': 500}. Best is trial 3 with value: 112301196.54020809.
    [I 2022-11-07 15:57:04,956] Trial 8 finished with value: 47582425.81597208 and parameters: {'noise': 5696.634895750645, 'n': 500}. Best is trial 3 with value: 112301196.54020809.
    [I 2022-11-07 15:57:05,420] Trial 9 finished with value: 25898857.90976508 and parameters: {'noise': 5105.352989948937, 'n': 500}. Best is trial 3 with value: 112301196.54020809.
    [I 2022-11-07 15:57:05,886] Trial 10 finished with value: 44545213.583875455 and parameters: {'noise': 6508.861504501792, 'n': 250}. Best is trial 3 with value: 112301196.54020809.

    112301196.54020809

``` python
class AddC (Component):
    def __init__ (self, c=0, **kwargs):
        super ().__init__ (**kwargs, labels_included_without_fitting=True)
        self.c = c
    def _apply (self, X):
        return X+self.c
```

## Documentation

For further details, please see the
[documentation](https://jaume-jci.github.io/ds-blocks/)
