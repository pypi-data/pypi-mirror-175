import random
import warnings

import numpy as np
import pandas as pd
import torch
from pytorch_forecasting.data.timeseries import (
    check_for_nonfinite, _find_end_indices
)
from torch import nn
from torch.nn.utils import rnn

from ._prediction_decoders import SeqToSeqPredictionDecoder
from .base import BaseTrainer, BaseModule, BaseDataset
from ..preprocessing import MultiColumnLabelEncoder


class SeqToSeq(BaseTrainer):
    """SeqToSeq architecture with additional Embedding layer.

    This model applies a sequence to sequence learning architecture to solve
    the multivariate multistep time series forecasting problem. The additional
    embedding layer allows to condition the encoder module on
    time independent/static categorical data, making it possible to learn and
    predict multiple time series using a single model (i.e. a single non linear
    mapping).

    Parameters
    ----------
    group_ids : list of str
        List of column names identifying a time series. This means that the
        ``group_ids`` identify a sample together with the ``date``. If you
        have only one times series, set this to the name of column that is
        constant.

    time_idx : str
        Time index column. This column is used to determine the sequence of
        samples.

    target : str
        Target column. Column containing the values to be predicted.

    max_prediction_length : int
        Maximum prediction/decoder length. Usually this this is defined by
        the difference between forecasting dates.

    max_encoder_length : int
        Maximum length to encode. This is the maximum history length used by
        the time series dataset.

    time_varying_known_reals : list of str
        List of continuous variables that change over time and are known in the
        future (e.g. price of a product, but not demand of a product). If None,
        every numeric column excluding ``target`` is used.

    time_varying_unknown_reals : list of str
        List of continuous variables that change over time and are not known in
        the future. You might want to include your ``target`` here. If None,
        only ``target`` is used.

    static_categoricals : list of str
        List of categorical variables that do not change over time (also known
        as `time independent variables`). You might want to include your
        ``group_ids`` here for the learning algorithm to distinguish between
        different time series. If None, only ``group_ids`` is used.

    min_encoder_length : int, default=None
        Minimum allowed length to encode. If None, defaults to
        ``max_encoder_length``.

    criterion : class, default=None
        The uninitialized criterion (loss) used to optimize the module. If
        None, the :class:`.RMSE` (root mean squared error) is used.

    optimizer : class, default=None
        The uninitialized optimizer (update rule) used to optimize the
        module. if None, :class:`.Adam` optimizer is used.

    lr : float, default=1e-5
        Learning rate passed to the optimizer.

    max_epochs : int, default=10
        The number of epochs to train for each :meth:`fit` call. Note that you
        may keyboard-interrupt training at any time.

    batch_size : int, default=64
        Mini-batch size. If ``batch_size`` is -1, a single batch with all the
        data will be used during training and validation.

    callbacks: None, “disable”, or list of Callback instances, default=None
        Which callbacks to enable.

        - If callbacks=None, only use default callbacks which include:
            - `epoch_timer`: measures the duration of each epoch
            - `train_loss`: computes average of train batch losses
            - `valid_loss`: computes average of valid batch losses
            - `print_log`:  prints all of the above in nice format

        - If callbacks="disable":
            disable all callbacks, i.e. do not run any of the callbacks.

        - If callbacks is a list of callbacks:
            use those callbacks in addition to the default callbacks. Each
            callback should be an instance of skorch :class:`.Callback`.

    emb_dim : int, default=10
        Dimension of every embedding table

    hidden_size : int, default=16
        Size of the context vector

    tf_ratio : float, default=0.2
        For every forward pass, if the sampling from a standard uniform
        distribution is less than ``tf_ratio``, teacher forcing is used.

    cell_type : str, {'lstm', 'gru}, default='lstm'
        Recurrent unit to be used for both encoder and decoder

    verbose : int, default=1
        This parameter controls how much print output is generated by
        the net and its callbacks. By setting this value to 0, e.g. the
        summary scores at the end of each epoch are no longer printed.
        This can be useful when running a hyperparameter search. The
        summary scores are always logged in the history attribute,
        regardless of the verbose setting.

    device : str, torch.device, default="cpu"
        The compute device to be used. If set to "cuda", data in torch
        tensors will be pushed to cuda tensors before being sent to the
        module. If set to None, then all compute devices will be left
        unmodified.

    kwargs : dict
       Extra prefixed parameters (see list of supported prefixes with
       self.prefixes).
    """

    def __init__(self, group_ids, time_idx, target, max_prediction_length,
                 max_encoder_length, time_varying_known_reals,
                 time_varying_unknown_reals, static_categoricals,
                 cv_split=None, min_encoder_length=None, criterion=None,
                 optimizer=None, lr=1e-5, max_epochs=10, batch_size=64,
                 callbacks=None, emb_dim=10, hidden_size=16, tf_ratio=0.2,
                 cell_type='lstm', verbose=1, device='cpu', **kwargs):
        super().__init__(
            module=SeqToSeqModule, dataset=SeqToSeqDataset,
            group_ids=group_ids, time_idx=time_idx, target=target,
            max_prediction_length=max_prediction_length,
            time_varying_known_reals=time_varying_known_reals,
            time_varying_unknown_reals=time_varying_unknown_reals,
            static_categoricals=static_categoricals, cv_split=cv_split,
            max_encoder_length=max_encoder_length,
            min_encoder_length=min_encoder_length,
            collate_fn=SeqToSeqDataset.collate_fn,
            max_epochs=max_epochs, lr=lr, batch_size=batch_size,
            optimizer=optimizer, criterion=criterion, callbacks=callbacks,
            verbose=verbose, device=device,
            prediction_decoder=SeqToSeqPredictionDecoder,
            **kwargs
        )
        self.emb_dim = emb_dim
        self.hidden_size = hidden_size
        self.tf_ratio = tf_ratio
        self.cell_type = cell_type


class SeqToSeqModule(BaseModule):
    """Wrapper for the encoder decoder architecture with additional embedding
    layer.

    Parameters
    ----------
    emb_dim : int
        Dimension for every embedding table

    emb_sizes : tuple
        Size of each embedding table. For example, if two static categorical
        features are to be included to the model and they have `n` and `m`
        unique elements, respectively, then ``emb_sizes`` must equal `(n, m)`.
        The dimension of each embedding table is given by the ``emb_dim``
        param.

    enc_size : int
        Encoder size. Number of features to be included in the encoder module

    dec_size : int
        Decoder size. Number of feature to be included in the decoder module

    hidden_size : int
        Size of the context vector

    tf_ratio : float, default=0.2
        For every forward pass, if the sampling from a standard uniform
        distribution is less than ``tf_ratio``, teacher forcing ratio will be
        used.

    cell_type : str, {'lstm', 'gru}, default='lstm'
    """

    def __init__(self, emb_dim, emb_sizes, enc_size, dec_size, hidden_size,
                 tf_ratio=0.2, cell_type='lstm'):
        super().__init__()
        self.embedding = _Embedding(emb_sizes, emb_dim, hidden_size)
        self.encoder = _Encoder(enc_size, hidden_size, cell_type)
        self.decoder = _Decoder(dec_size, hidden_size, tf_ratio, cell_type)

    def forward(self, emb_x, enc_x, dec_x, dec_y, enc_lens):
        enc_state = self.embedding(emb_x)
        enc_out, enc_state = self.encoder(enc_x, enc_lens, h_0=enc_state)
        dec_state = enc_state  # Decoders state is last encoders state
        return self.decoder(dec_x, dec_state, enc_out, dec_y)

    @classmethod
    def from_dataset(cls, dataset, **kwargs):
        """Create model from dataset.

        Parameters
        ----------
        dataset : MasterDataset object
            dataset object from which init parameters will be obtained

        kwargs : keyword arguments
            additional arguments such as hyperparameters for model
            (see ``__init__()``)

        Returns
        -------
        SeqToSeqModule object
        """
        emb_sizes = tuple(
            torch.max(dataset.data['categoricals'], dim=0).values.numpy() + 1
        )
        enc_size = len(
            dataset.time_varying_known_reals +
            dataset.time_varying_unknown_reals
        )
        dec_size = len(dataset.time_varying_known_reals) + 1
        init_kwargs = dict(
            emb_sizes=emb_sizes, enc_size=enc_size, dec_size=dec_size
        )
        init_kwargs.update(**kwargs)
        return cls(**init_kwargs)


class _Embedding(nn.Module):
    """Embedding layers for conditioning the encoder with time
    independents/static categorical variables.
    """

    def __init__(self, emb_sizes, emb_dim, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.emb_sizes = emb_sizes
        emb_trans = {}
        linear_trans = {}
        self.linear = nn.Linear(len(emb_sizes), 1)
        for i, size in enumerate(emb_sizes):
            # Each column will have its own embedding and linear
            # transformation. The order will be kept through dictionaries.
            emb_trans[str(i)] = nn.Embedding(size, emb_dim)
            linear_trans[str(i)] = nn.Linear(emb_dim, hidden_size)
        self.emb_trans = nn.ModuleDict(emb_trans)
        self.linear_trans = nn.ModuleDict(linear_trans)

    def forward(self, tokens):
        batch_size = tokens.shape[0]
        # ``dense_conditionals`` concatenates all static (time independent)
        # conditions after they have been embedded and linearly transformed.
        output_shape = (batch_size, self.hidden_size, len(self.emb_sizes))
        dense_conditionals = torch.zeros(output_shape, device=tokens.device)
        for i, col in enumerate(self.emb_trans):
            column_tokens = tokens[:, [i]]
            emb_column = self.emb_trans[col](column_tokens)
            dense_column = self.linear_trans[col](emb_column)
            dense_conditionals[:, :, i] = dense_column.squeeze(dim=1)
        if len(self.emb_sizes) > 1:
            output = self.linear(dense_conditionals)
        else:
            output = dense_conditionals
        output = output.permute(2, 0, 1)
        return output


class _Encoder(nn.Module):
    """Encoder module
    """

    def __init__(self, input_size, hidden_size, cell_type):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.cell_type = cell_type
        self.cell_unit = _cell_selector(self)

    def forward(self, input, enc_lens, h_0=None):
        if self.cell_type == 'lstm':
            h_0 = self._lstm_h_0(h_0)
        input_packed = rnn.pack_padded_sequence(
            input, enc_lens, batch_first=True, enforce_sorted=False
        )
        output_packed, state = self.cell_unit(input_packed, h_0)
        output, _ = rnn.pad_packed_sequence(output_packed, batch_first=True)
        return output, state

    def _lstm_h_0(self, h_0=None):
        """Initializes lstm hidden tensors h_0 and c_0
        """
        if h_0 is None:
            # If the initial hidden state h_0 is None, a None type is returned
            # to trigger the default behaviour for both h_0 and c_0 (both h_0
            # and c_0 default to zero)
            return None
        # Otherwise, h_0 is left untouched and only the initial cell state c_0
        # is set to zero.
        c_0 = torch.zeros_like(h_0, device=h_0.device)
        return h_0, c_0


class _Decoder(nn.Module):
    """Decoder module

    Parameters
    ----------
    input_size : int
        Decoder input size (number of features)

    hidden_size : int
        Size of context vector

    tf_ratio : float
        For each forward pass, if a random sample from a standard uniform
        distribution is less than `tf_ratio`, teacher forcing is used. Use 0
        if teacher forcing is not desired at all.

    cell_type : str, {'lstm', 'gru'}
        Rnn cell for encoder and decoder
    """

    def __init__(self, input_size, hidden_size, tf_ratio, cell_type):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.cell_type = cell_type
        self.cell_unit = _cell_selector(self)
        self.tf_ratio = tf_ratio
        self.linear = nn.Linear(in_features=hidden_size, out_features=1)

    def forward(self, dec_x, state, enc_out, y):
        # Teacher forcing is only available when model.train() is active.
        if self._tf() and self.training:
            return self._tf_forward(y, dec_x, state)
        return self._notf_forward(enc_out, dec_x, state)

    def _tf(self):
        """Sampling from the standard uniform random to decide whether or not
        teacher forcing is used

        Returns
        -------
            - True if a random sample from a standard uniform distribution is
            less than self.tf_ratio.
            - False otherwise.
        """
        tf = True if random.random() < self.tf_ratio else False
        return tf

    def _tf_forward(self, y, dec_x, state):
        """Target values are used as input at every step.

        This is achieved by concatenating the target values in ``y`` and the
        features values in ``dec_x`` in a single input tensor.
        """
        # ``y`` is a tensor of size (batch_size, T) where T is
        # the prediction length and ``dec_x`` is a tensor of size
        # (batch_size, T, F) where F is the number of features. Therefore,
        # an extra dimension is added to ``y`` to perform torch.cat and produce
        # the cell input.
        dec_input = torch.cat((y.unsqueeze(dim=2), dec_x), dim=2)
        dec_output, state = self.cell_unit(dec_input, state)
        dec_output = self.linear(dec_output)
        return dec_output.squeeze(-1)  # squeeze last dim from linear output

    def _notf_forward(self, enc_out, dec_x, state):
        """Output values are used as next input at every step with the first
        input being the last encoder output.
        """
        batch_size = enc_out.shape[0]
        output_length = dec_x.shape[1]
        # First decoder input is the last encoder output
        dec_input_t = enc_out[:, -1:, -1:]
        # decoder_output is a holder for the outputs at every timestep
        dec_output = torch.zeros(
            (batch_size, output_length, 1), device=dec_x.device
        )
        for t in range(output_length):
            future_seqs_t = dec_x[:, [t], :]
            dec_input_t = torch.cat((dec_input_t, future_seqs_t), dim=2)
            dec_output_t, state = self.cell_unit(dec_input_t, state)
            dense_dec_output_t = self.linear(dec_output_t)
            dec_output[:, t, :] = dense_dec_output_t.squeeze(1)
            dec_input_t = dense_dec_output_t.detach()
        return dec_output.squeeze(-1)  # squeeze last dim from linear output


def _cell_selector(module):
    """Private function for selecting between lstm and gru cells.
    """
    if module.cell_type == 'lstm':
        cell_unit = nn.LSTM(
            input_size=module.input_size,
            hidden_size=module.hidden_size,
            num_layers=1,
            batch_first=True
        )
    elif module.cell_type == 'gru':
        cell_unit = nn.GRU(
            input_size=module.input_size,
            hidden_size=module.hidden_size,
            num_layers=1,
            batch_first=True
        )
    else:
        raise ValueError(f'{module.cell_type} is not a valid recurrent unit')
    return cell_unit


class SeqToSeqDataset(BaseDataset):
    """Dataset for SeqToSeq model

    In brief, this is a light weight version of :class:`TimeSeriesDataset` from
    the pytorch_forecasting library.

    Parameters
    ----------
    X : pd.DataFrame
        Dataframe with time series data. Each row can be identified with
        ``date`` and the ``group_ids``.

    group_ids : list of str
        List of column names identifying a time series. This means that the
        ``group_ids`` identify a sample together with ``date``. If you
        have only one times series, set this to the name of column that is
        constant.

    time_idx : str
        Time index column. This column is used to determine the sequence of
        samples.

    target : str
        Target column. Column containing the values to be predicted.

    max_prediction_length : int
        Maximum prediction/decoder length. Usually this is defined by the
        difference between forecasting dates.

    max_encoder_length : int, default=None
        Maximum length to encode (also known as `input sequence length`). This
        is the maximum history length used by the time series dataset.

    time_varying_known_reals : list of str
        List of continuous variables that change over time and are known in the
        future (e.g. price of a product, but not demand of a product).

    time_varying_unknown_reals : list of str
        List of continuous variables that change over time and are not known in
        the future. You might want to include your ``target`` here.

    static_categoricals : list of str, default=None
        List of categorical variables that do not change over time (also known
        as `time independent variables`). If None, ``group_ids`` will be used.

    min_encoder_length : int, default=None
        Minimum allowed length to encode. If None, defaults to
        ``max_encoder_length``.

    categorical_encoder : MultiColumnLabelEncoder object, default=None

    predict_mode : bool, default=False

    add_missing_sequences : bool, default=False
        The missing_sequences ensure that there is a sequence that finishes on
        every timestep. Warning: might drastically increase the length of the
        dataset.

    Attributes
    ----------
    data : dict of tensors
        Dictionary containing the data in ``X`` divided in five groups:
            - reals: continuous data (includes target)
            - categoricals: encoded categorical data
            - target: target data
            - groups:  encoded group ids
            - time: time index
    """

    def __init__(self, X, group_ids, time_idx, target, max_prediction_length,
                 max_encoder_length, time_varying_known_reals,
                 time_varying_unknown_reals, static_categoricals=None,
                 min_encoder_length=None, categorical_encoder=None,
                 predict_mode=False, add_missing_sequences=False):

        super().__init__()
        self.group_ids = group_ids
        self.time_idx = time_idx
        self.target = target
        self.max_prediction_length = max_prediction_length
        self.max_encoder_length = max_encoder_length
        self.time_varying_known_reals = time_varying_known_reals
        self.time_varying_unknown_reals = time_varying_unknown_reals
        self.static_categoricals = static_categoricals
        self._validate_columns(X)
        self.categorical_encoder = categorical_encoder
        if min_encoder_length is None:
            min_encoder_length = max_encoder_length
        self.min_encoder_length = min_encoder_length
        self.min_prediction_length = max_prediction_length
        self.predict_mode = predict_mode
        self.add_missing_sequences = add_missing_sequences

        # Preprocess X
        X = self._preprocess_X(X)
        self.min_prediction_idx = X[self.time_idx].min()
        self.index = self._construct_index(X)
        self.data = self._X_to_tensors(X)

    def calculate_decoder_length(
            self,
            time_last,
            sequence_length,
    ):
        """Calculates length of decoder.

        Parameters
        ----------
        time_last : Union[int, pd.Series, np.ndarray]
            Last time index of the sequence.

        sequence_length : Union[int, pd.Series, np.ndarray]
            Total length of the sequence.

        Returns
        -------
        decoded_length(s) : Union[int, pd.Series, np.ndarray]
        """
        if isinstance(time_last, int):
            decoder_length = min(
                time_last - (self.min_prediction_idx - 1),
                # not going beyond min prediction idx
                self.max_prediction_length,  # maximum prediction length
                sequence_length - self.min_encoder_length,
                # sequence length - min decoder length
            )
        else:
            decoder_length = np.min(
                [
                    time_last - (self.min_prediction_idx - 1),
                    sequence_length - self.min_encoder_length,
                ],
                axis=0,
            ).clip(max=self.max_prediction_length)
        return decoder_length

    def __getitem__(self, idx):
        """Returns a single dataset sample
        """
        # Get data
        # --------
        index = self.index.iloc[idx]
        data_cont = self.data["reals"][
                    index.index_start: index.index_end + 1].clone()
        data_cat = self.data["categoricals"][
                   index.index_start: index.index_end + 1].clone()

        time = self.data["time"][
               index.index_start: index.index_end + 1].clone()

        # Determine data window
        # ---------------------
        sequence_length = len(time)
        assert (
                sequence_length >= self.min_prediction_length
        ), "Sequence length should be at least minimum prediction length"

        # Determine prediction/decode length and encode length.
        decoder_length = self.calculate_decoder_length(
            time_last=time[-1],
            sequence_length=sequence_length
        )
        encoder_length = sequence_length - decoder_length

        # Decoder and encoder assertions.
        assert (
                decoder_length >= self.min_prediction_length
        ), "Decoder length should be at least minimum prediction length"
        assert encoder_length >= self.min_encoder_length, (
            "Encoder length should be at least minimum encoder length"
        )

        # Split data for model input
        # --------------------------
        # Split continuous data into encoder and decoder inputs.
        # ``data_cont`` contains the n time_varying_unknown features
        # at the end (see ``_X_to_tensors`` method). Since the decoder only
        # works with known data, we must exclude them.
        n = len(self.time_varying_unknown_reals)
        enc_x = data_cont[:encoder_length, :]
        dec_x = data_cont[-decoder_length:, :-n]
        emb_x = data_cat.unique(dim=0).squeeze(-1)

        # Get target values. ``y`` contains both encoder and decoder target
        # values. However, only decoder target values are used.
        y = [
            d[index.index_start: index.index_end + 1].clone()
            for d in self.data["target"]
        ]
        dec_y = y[0][encoder_length:]

        # Return (X, y) tuple where X is a dict with all the model inputs.
        # Notice target values ``dec_y`` are also returned as input since
        # teacher forcing is possible.
        return dict(emb_x=emb_x, enc_x=enc_x, dec_x=dec_x, dec_y=dec_y), dec_y

    def __len__(self):
        return len(self.index)

    @staticmethod
    def collate_fn(batches):
        """collate_fn for padding variable length sequences

        SeqToSeq only allows encoder input to be variable length.
        """
        emb_x = torch.stack([b[0]["emb_x"] for b in batches]).view(
            len(batches), -1)
        enc_lens = [len(b[0]['enc_x']) for b in batches]
        enc_x_pad = rnn.pad_sequence(
            [b[0]["enc_x"] for b in batches], batch_first=True
        )
        dec_x = torch.stack([b[0]["dec_x"] for b in batches])
        dec_y = torch.stack([b[0]["dec_y"] for b in batches])

        return dict(
            emb_x=emb_x,
            enc_x=enc_x_pad,
            dec_x=dec_x,
            dec_y=dec_y,
            enc_lens=enc_lens,
        ), dec_y

    def _validate_columns(self, X):
        """Checks presence of columns in X and guarantees
        ``static_categoricals`` include ``group_ids``
        """
        # check presence in ``X``
        for col in self.time_varying_known_reals:
            if col not in X:
                raise ValueError(
                    'column {} from time_varying_known_reals was not found '
                    'in X'.format(col)
                )
        for col in self.time_varying_unknown_reals:
            if col not in X:
                raise ValueError(
                    'column {} from time_varying_unknown_reals was not found '
                    'in X'.format(col)
                )
        # ``static_categoricals always have to include ``group_ids``
        if self.static_categoricals is not None:
            for g in self.group_ids:
                if g not in self.static_categoricals:
                    self.static_categoricals.append(g)
            for col in self.static_categoricals:
                if col not in X:
                    raise ValueError(
                        'column {} from static_categoricals was not found '
                        'in X'.format(col)
                    )
        else:
            self.static_categoricals = self.group_ids

    def _preprocess_X(self, X):
        """Encodes static categoricals
        """
        # Encode static categoricals
        if self.categorical_encoder is None:
            self.categorical_encoder = MultiColumnLabelEncoder(
                self.static_categoricals).fit(X)
        X = self.categorical_encoder.transform(X)

        # Sort and reset
        pk = self.group_ids + [self.time_idx]  # primary key
        X = X.sort_values(pk).reset_index(drop=True)
        return X

    def _construct_index(self, X):
        """Creates index of samples.

        Parameters
        ----------
        X : pd.DataFrame
            Categorically encoded X

        Returns
        -------
        pd.DataFrame: index dataframe
        """
        g = X.groupby(self.group_ids, observed=True)

        df_index_first = g[self.time_idx].transform("nth", 0).to_frame(
            "time_first")
        df_index_last = g[self.time_idx].transform("nth", -1).to_frame(
            "time_last")
        df_index_diff_to_next = -g[self.time_idx].diff(-1).fillna(-1).astype(
            int).to_frame("time_diff_to_next")
        df_index = pd.concat(
            [df_index_first, df_index_last, df_index_diff_to_next], axis=1)
        df_index["index_start"] = np.arange(len(df_index))
        df_index["time"] = X[self.time_idx]
        df_index["count"] = (df_index["time_last"] - df_index[
            "time_first"]).astype(int) + 1
        group_ids = g.ngroup()
        df_index["group_id"] = group_ids

        min_sequence_length = (
                self.min_prediction_length + self.min_encoder_length
        )
        max_sequence_length = (
                self.max_prediction_length + self.max_encoder_length
        )

        # Calculate maximum index to include from current index_start
        max_time = (df_index["time"] + max_sequence_length - 1).clip(
            upper=df_index["count"] + df_index.time_first - 1)

        df_index["index_end"], missing_sequences = _find_end_indices(
            diffs=df_index.time_diff_to_next.to_numpy(),
            max_lengths=(max_time - df_index.time).to_numpy() + 1,
            min_length=min_sequence_length,
        )

        # Add duplicates but mostly with shorter sequence length for start of
        # timeseries. While the previous steps have ensured that we start a
        # sequence on every time step, the missing_sequences
        # ensure that there is a sequence that finishes on every timestep
        if len(missing_sequences) > 0 and self.add_missing_sequences:
            shortened_sequences = df_index.iloc[
                missing_sequences[:, 0]].assign(
                index_end=missing_sequences[:, 1])

            # Concatenate shortened sequences
            df_index = pd.concat([df_index, shortened_sequences], axis=0,
                                 ignore_index=True)

        # Filter out where encode and decode length are not satisfied
        df_index["sequence_length"] = df_index["time"].iloc[
                                          df_index["index_end"]
                                      ].to_numpy() - df_index["time"] + 1

        # Filter too short sequences
        df_index = df_index[
            # Sequence must be at least of minimal prediction length
            lambda x: (x.sequence_length >= min_sequence_length)
                      &
                      # Prediction must be for after minimal prediction index +
                      # length of prediction
                      (x["sequence_length"] + x["time"] >=
                       self.min_prediction_idx + self.min_prediction_length)
        ]

        # Check that all groups/series have at least one entry in the index
        if not group_ids.isin(df_index.group_id).all():
            missing_groups = X.loc[~group_ids.isin(
                df_index.group_id), self.group_ids].drop_duplicates()
            warnings.warn(
                "Min encoder length and/or min_prediction_idx and/or min "
                "prediction length and/or lags are too large for "
                "{} series/groups which therefore are not "
                "present in the dataset index. This means no predictions can "
                "be made for those series. "
                "First 10 removed groups: {}".format(
                    len(missing_groups),
                    list(
                        missing_groups.iloc[:10].to_dict(
                            orient='index').values()
                    )
                )
            )
        assert (
                len(df_index) > 0
        ), ("Min encoder length and/or and/or max encoder length and/or "
            "max prediction length are too large for ALL groups")

        # Predict mode
        if self.predict_mode:
            # Get the rows containing the max sequence length of their group.
            # Note that if a group has multiple max values, all will be
            # returned.
            max_on_each_row = df_index.groupby('group_id')[
                'sequence_length'].transform(max)
            idx = (max_on_each_row == df_index['sequence_length'])
            df_index = df_index.loc[idx].reset_index(drop=True)
        else:
            df_index = df_index.reset_index(drop=True)

        return df_index

    @property
    def decoded_index(self):
        """Get interpretable version of index.

        DataFrame contains
        - group_id columns in original encoding
        - time_idx_first column: first time index of subsequence.
        - time_idx_last columns: last time index of subsequence.
        - time_idx_first_prediction columns: first time index which is in
        decoder.

        Returns
        -------
        decoded_index : pd.DataFrame
            Index that can be understood in terms of original data
        """
        index_start = self.index["index_start"].to_numpy()
        index_last = self.index["index_end"].to_numpy()

        # Get group ids original values in order of index.
        group_ids_df = self.categorical_encoder.inverse_transform(
            pd.DataFrame(
                data=self.data["groups"][index_start].numpy(),
                columns=self.group_ids
            )
        )

        return group_ids_df.assign(
            time_idx_first=self.data["time"][index_start].numpy(),
            time_idx_last=self.data["time"][index_last].numpy(),
            time_idx_first_prediction=
            lambda x: x.time_idx_last - self.calculate_decoder_length(
                time_last=x.time_idx_last,
                sequence_length=x.time_idx_last - x.time_idx_first + 1) + 1
        )

    def _X_to_tensors(self, X):
        """Convert data to tensors for faster access with :meth:`__getitem__`.

        Parameters
        ----------
        X : pd.DataFrame
            Categorically encoded X

        Returns
        -------
        dict of tensors
            Dictionary of tensors for continuous, categorical data, groups,
            target and time index
        """
        groups = check_for_nonfinite(
            torch.tensor(X[self.group_ids].to_numpy(np.long),
                         dtype=torch.long), self.group_ids
        )
        time = check_for_nonfinite(
            torch.tensor(X[self.time_idx].to_numpy(np.long),
                         dtype=torch.long), self.time_idx
        )
        categorical = check_for_nonfinite(
            torch.tensor(
                X[self.static_categoricals].to_numpy(np.long),
                dtype=torch.long
            ),
            self.static_categoricals
        )
        target = [
            check_for_nonfinite(
                torch.tensor(
                    X[self.target].to_numpy(dtype=np.float),
                    dtype=torch.float),
                self.target,
            )
        ]
        reals = self.time_varying_known_reals + self.time_varying_unknown_reals
        continuous = check_for_nonfinite(
            torch.tensor(X[reals].to_numpy(dtype=np.float), dtype=torch.float),
            reals
        )

        tensors = dict(
            reals=continuous,
            categoricals=categorical,
            target=target,
            groups=groups,
            time=time
        )
        return tensors
