from itertools import chain
from warnings import simplefilter
from copy import deepcopy

from numpy.random import choice
from numpy import cumsum, array, unique, isnan, nan_to_num, log, exp, mean, dtype, concatenate
from pandas import to_timedelta, DataFrame, Series
from scipy.stats import gaussian_kde

from sklearn.exceptions import ConvergenceWarning
from sklearn.mixture import GaussianMixture

simplefilter(category=ConvergenceWarning, action="ignore")


class Simulation:
    """
    Class that performs process simulation (as-is) and what-if analysis (to-be).
    Note: the execution time is given in minutes.

    Parameters
    ----------
    data_holder: DataHolder
        Object that contains the event log and names of its necessary columns (id, activities, timestamps, etc.).

    Examples
    --------
    >>> from sberpm.imitation import Simulation
    >>> sim = Simulation(sberpm.data_holder)
    >>> sim_data = sim.run(num_of_traces=800)
    """

    def __init__(self, data_holder):
        data_holder.check_or_calc_duration()
        data_holder.get_grouped_data(data_holder.activity_column, data_holder.duration_column)

        self._supp_data = self._add_end_event(data_holder)
        self.starting_acts_prob = self._get_starting__activities_prob(data_holder)
        self.node_node_prob = self._get_node_node_cond_prob(data_holder)
        self.edge_node_prob = self._get_edge_node_cond_prob(data_holder)
        self.edge_node_node_prob = self._get_edge_node_node_cond_prob(data_holder)
        self._transitions_duration = self._get_transitions_duration(data_holder)
        self._approx_transitions_duration = {}
        self.start_timestamps = self._calc_inter_arrival_rate(data_holder)
        self._activities_duration = self._sample_approx_activities_duration(data_holder)
        self._reserved_node_node_prob = deepcopy(self.node_node_prob)
        self._reserved_edge_node_prob = deepcopy(self.edge_node_prob)
        self._reserved_edge_node_node_prob = deepcopy(self.edge_node_node_prob)
        self._reserved_starting_acts_prob = deepcopy(self.starting_acts_prob)
        self._reserved_activities_durations = deepcopy(self._activities_duration)
        self.holder = data_holder
        self.generated_log = None
        self._reserved_transitions_durations = None

    def run(self, num_of_traces):

        """
        Builds a simulation model and generates an event log with a given number of event traces.

        Parameters
        ----------

        num_of_traces: int
            Number of event traces (unique ids) to simulate.

        Returns
        -------
        df: DataFrame
            Data generated by the simulation model.
        """

        self.generated_log = []

        for i in range(num_of_traces):
            activities_sequence = []
            activities_duration_sequence = []
            start_timestamps = []
            activities_list, durations_list = self._recurs_choice(
                self.starting_acts_prob.index,
                self.starting_acts_prob.values,
                activities_sequence,
                activities_duration_sequence,
            )

            if i == 0:
                start_timestamps.append(sorted(self.holder.data[self.holder.start_timestamp_column])[0])
            else:
                start_timestamps.append(
                    self.generated_log[i - 1]["start_timestamp_column"][0]
                    + to_timedelta(choice(self.start_timestamps), unit="s", errors="coerce")
                )
            start_timestamps.extend(to_timedelta(durations_list[:-1], unit="s", errors="coerce"))
            start_timestamps = cumsum(start_timestamps)
            self.generated_log.append(
                dict(
                    id_column=i,
                    activity_column=activities_list,
                    duration_column=durations_list,
                    start_timestamp_column=start_timestamps,
                )
            )
        self.generated_log = DataFrame(self.generated_log)
        self.generated_log.rename(
            columns={
                "id_column": self.holder.id_column,
                "activity_column": self.holder.activity_column,
                "duration_column": self.holder.duration_column,
                "start_timestamp_column": self.holder.start_timestamp_column,
            },
            inplace=True,
        )
        self.generated_log = (
            self.generated_log.set_index([self.holder.id_column]).apply(Series.explode).reset_index()
        )

        if not self._reserved_transitions_durations:
            self._reserved_transitions_durations = deepcopy(self._approx_transitions_duration)

        return self.generated_log

    def _recurs_choice(self, start_node, start_prob, activities_sequence_, activities_duration_sequence_):
        """
        Recursively constructs an event trace and a chain of its activities' time durations.

        Parameters
        ----------
        start_node: array-like of str
            Activities one of which might be chosen to be the next one.

        start_prob: numpy.array of float
            Probabilities of the given activities to be chosen the next ones.

        activities_sequence_: list of str
            Trace that a newly chosen activity will be concatenated to.

        activities_duration_sequence_: list of float
            Time durations of the activities in a simulated trace.

        Returns
        -------
        activities_sequence_: list of str
            Simulated event trace.

        activities_durations_sequence_: list of float
            Time durations of the activities in a simulated trace.

        """
        new_activity = choice(start_node, p=start_prob)
        if new_activity == "end":
            target_pair = (activities_sequence_[-1], new_activity)
            if target_pair not in self.edge_node_prob or (
                    target_pair not in self._transitions_duration
                    and target_pair not in self._approx_transitions_duration
            ):
                activities_duration_sequence_.append(choice(self._activities_duration[activities_sequence_[-1]]))
            else:
                activities_duration_sequence_.append(choice(self._sample_approx_transitions_duration(target_pair)))
            return activities_sequence_, activities_duration_sequence_
        activities_sequence_.append(new_activity)

        if len(activities_sequence_) > 1:
            target_pair = (activities_sequence_[-2], activities_sequence_[-1])
            if target_pair not in self.edge_node_prob or (
                    target_pair not in self._transitions_duration
                    and target_pair not in self._approx_transitions_duration
            ):
                activities_duration_sequence_.append(choice(self._activities_duration[new_activity]))
            else:
                activities_duration_sequence_.append(choice(self._sample_approx_transitions_duration(target_pair)))

        if len(activities_sequence_) < 2 or target_pair not in self.edge_node_prob:
            next_acts = list(self.node_node_prob[new_activity].index)
            next_probs = array(self.node_node_prob[new_activity].values)

        else:
            next_acts = list(self.edge_node_prob[tuple(activities_sequence_[-2:])].index)
            next_probs = array(self.edge_node_prob[tuple(activities_sequence_[-2:])].values)

        return self._recurs_choice(next_acts, next_probs, activities_sequence_, activities_duration_sequence_)

    def _get_transitions_duration(self, data_holder):
        """
        Get the durations of all possible transitions from the original log file.
        :param data_holder:sberpm.DataHolder

        :return:
        transitions_durations: dict of {(str, str): numpy.array of float}
            Key: transition, value: array of probable time durations.
        """
        transitions_durations = DataFrame()
        transitions_durations[data_holder.activity_column] = list(
            zip(
                self._supp_data[data_holder.activity_column],
                self._supp_data[data_holder.activity_column].shift(-1),
            )
        )
        transitions_durations[data_holder.duration_column] = self._supp_data[data_holder.duration_column]
        id_change_mask = array(
            self._supp_data[data_holder.id_column].shift(-1) != (self._supp_data[data_holder.id_column])
        )
        transitions_durations[id_change_mask] = None
        transitions_durations.dropna(inplace=True)
        transitions_durations = (
            transitions_durations.groupby(data_holder.activity_column)
                .agg({data_holder.duration_column: tuple})
                .reset_index()
        )
        transitions_durations = transitions_durations.set_index(data_holder.activity_column).T.to_dict("list")
        return transitions_durations

    def _sample_approx_transitions_duration(self, target_pair):
        """
        Calculate approximate durations of given transition.

        Parameters
        ----------
        target_pair: tuple :(str,str)

        Returns
        -------
        self._approx_transitions_duration[target_pair]:
            approximate durations of given transitions
        """
        if target_pair not in self._approx_transitions_duration:
            if (
                    (len(self._transitions_duration[target_pair][0]) > 1)
                    and (len(unique(self._transitions_duration[target_pair][0])) > 3)
                    and not all(isnan(self._transitions_duration[target_pair][0]))
            ):
                X = log(array(self._transitions_duration[target_pair][0]) + 0.1)
                nan_to_num(X, copy=False, nan=0)
                kde = gaussian_kde(dataset=X, bw_method="scott")
                size = max(len(X), 1000)

                self._approx_transitions_duration[target_pair] = exp(kde.resample(size)[0])
            else:
                self._approx_transitions_duration[target_pair] = unique(
                    nan_to_num(self._transitions_duration[target_pair], copy=False, nan=0)
                )
        return self._approx_transitions_duration[target_pair]

    def _sample_approx_activities_duration(self, data_holder):
        """
        Calculate the approximate  duration of activities.

        Parameters
        ----------
        data_holder: sberpm.DataHolder

        Returns
        -------
        acts_duration: dict of {str: numpy.array of float}
            Key: node, value: array of probable time durations.
        """
        acts_duration = {}
        for activity in unique(self._supp_data[data_holder.activity_column]):
            df = self._supp_data[self._supp_data[data_holder.activity_column] == activity][
                data_holder.duration_column
            ]

            if not all(df.isna()) and len(df.value_counts()) > 1:
                X = log(df + 1)
                # TODO move to typed separate function
                X.fillna(X.mean(), inplace=True)
                X = array(X).reshape(-1, 1)
                gm = GaussianMixture(n_components=4, random_state=42).fit(X)
                acts_duration[activity] = exp(gm.sample(n_samples=len(X))[0].reshape(1, -1))[0]
            else:
                acts_duration[activity] = unique(nan_to_num(df, copy=False, nan=0))

        return acts_duration

    @staticmethod
    def _add_end_event(data_holder):
        """
        Adds 'end_event' (and its zero time duration) to the traces in the event log.

        Parameters
        ----------
        data_holder: sberpm.DataHolder

        Returns
        -------
        supp_data:pandas.DataFrame
            Modified log data with 'end_event', columns: [activity column (str), time duration column (float (minutes)]
        """

        supp_data = (
            data_holder.data.groupby(data_holder.id_column)
                .agg({data_holder.activity_column: tuple, data_holder.duration_column: tuple})
                .reset_index()
        )

        supp_data["act_end"] = [("end",)] * supp_data.shape[0]
        supp_data["time_end"] = [(0,)] * supp_data.shape[0]
        supp_data[data_holder.activity_column] = supp_data[data_holder.activity_column] + supp_data["act_end"]
        supp_data[data_holder.duration_column] = supp_data[data_holder.duration_column] + supp_data["time_end"]

        supp_data = (
            supp_data[[data_holder.id_column, data_holder.activity_column, data_holder.duration_column]]
                .apply(Series.explode)
                .reset_index(drop=True)
        )
        supp_data[data_holder.duration_column] = supp_data[data_holder.duration_column].fillna(0)

        return supp_data

    @staticmethod
    def _calc_inter_arrival_rate(data_holder):
        """
        Simulates inter-arrival rate: time durations between the beginnings of the neighbouring event traces.

        Parameters
        ----------
        data_holder: sberpm.DataHolder

        Returns
        -------
        start_timestamps: numpy.array of int, shape=[number of ids]
            Time durations in s.
        """
        start_timestamp_mask = data_holder.data[data_holder.id_column] != data_holder.data[
            data_holder.id_column
        ].shift(1)
        start_timestamps_s_numpy = (
            data_holder.data[start_timestamp_mask][data_holder.start_timestamp_column].dropna().values
        )
        start_timestamps_s_numpy.sort()
        start_timestamps_s = Series(start_timestamps_s_numpy)
        time_periods_s = start_timestamps_s.shift(-1) - start_timestamps_s
        time_periods_s.dropna(inplace=True)
        time_periods_s = time_periods_s.apply(lambda x: x.total_seconds())
        kde = gaussian_kde(dataset=log(time_periods_s + 0.1), bw_method="scott")

        return exp(kde.resample(len(time_periods_s))[0]).ravel()

    def _calc_new_transition_duration(self, first_edge, second_edge):
        """
        Samples time durations of the edge A->C based on mean
        of the known time durations of edges A->B and B->C.

        Parameters
        ----------
        first_edge: tuple(str, str)
            Edge 'A->B'.

        second_edge: tuple(str, str)
            Edge 'B->C'.
        """
        self._approx_transitions_duration[first_edge] = self._sample_approx_transitions_duration(first_edge)
        self._approx_transitions_duration[second_edge] = self._sample_approx_transitions_duration(second_edge)
        arr_1 = log(self._approx_transitions_duration[first_edge] + 1).reshape(-1, 1)
        arr_2 = log(self._approx_transitions_duration[second_edge] + 1).reshape(-1, 1)
        size = max(len(arr_1), len(arr_2))
        arr_1, n_c1 = self._get_approximation_parameters(arr_1, size)
        arr_2, n_c2 = self._get_approximation_parameters(arr_2, size)
        gm_1 = GaussianMixture(n_components=n_c1, random_state=42).fit(arr_1) if n_c1 > 0 else None
        gm_2 = GaussianMixture(n_components=n_c2, random_state=42).fit(arr_2) if n_c2 > 0 else None

        t_pair = (first_edge[0], second_edge[1])
        if gm_1:
            res_1 = exp(gm_1.sample(size)[0].reshape(1, -1))[0]
        else:
            res_1 = exp(arr_1)

        if gm_2:
            res_2 = exp(gm_2.sample(size)[0].reshape(1, -1))[0]
        else:
            res_2 = exp(arr_2)

        self._approx_transitions_duration[t_pair] = mean([res_1, res_2])
        if dtype(self._approx_transitions_duration[t_pair]) == "float64":
            self._approx_transitions_duration[t_pair] = int(self._approx_transitions_duration[t_pair])

    @staticmethod
    def _get_approximation_parameters(arr, size):
        """
        Gets an array of time durations of the appropriate size and
        the 'n_components' parameter for the GaussianMixture algorithm.

        Parameters
        ----------
        arr: array-like of float
        size: int

        Returns
        -------
        arr: array-like of float
            Possible modified array of time durations.

        n_comp: int
            'n_components' parameter for the GaussianMixture algorithm.
        """
        if len(arr) < 2:
            arr = list(arr[0]) * size

            n_comp = 0
            return arr, n_comp
        elif len(arr) < 3:
            arr = concatenate([arr, arr, arr])
        n_comp = 4 if len(arr) > 4 else len(arr) - 1

        return arr, n_comp

    def delete_node(self, node):
        """
        Deletes a node from the process.

        :param node:str
        :return:None
        """
        if node in self.starting_acts_prob:
            self.starting_acts_prob.drop(node, inplace=True)
            self.starting_acts_prob = self.starting_acts_prob / self.starting_acts_prob.sum()

        self._resolve_node_node_connections(node)
        self._resolve_edge_node_connections(node)
        for key in list(self.edge_node_node_prob):
            if node in self.edge_node_node_prob[key]:
                self.edge_node_node_prob[key].drop(node, inplace=True)
                self.edge_node_node_prob[key] = self.edge_node_node_prob[key] / self.edge_node_node_prob[key].sum()

    def delete_edge(self, edge):
        """
        Deletes the edge from the process.
        If the edge is the only outgoing edge from the source node, it will not be removed.

        :param edge:tuple of (str,str)
        :return:None
        """
        trigger = 0
        if len(self.node_node_prob[edge[0]]) == 1:
            print('Deletion of the only possible transition "', edge, '" is ignored')
            trigger = 1
        elif edge[1] in self.node_node_prob[edge[0]].index:
            self.node_node_prob[edge[0]].drop(edge[1], inplace=True)
            self.node_node_prob[edge[0]] = self.node_node_prob[edge[0]] / self.node_node_prob[edge[0]].sum()

        if edge in self.edge_node_prob and trigger != 1:
            del self.edge_node_prob[edge]

        for key in list(self.edge_node_prob):
            if key[1] == edge[0] and edge[1] in self.edge_node_prob[key] and trigger != 1:
                cycle_check = 0
                if key[0] == key[1]:
                    cycle_check = self._check_for_self_cycles(key[0], edge[1])
                self.edge_node_prob[key].drop(edge[1], inplace=True)
                self.edge_node_prob[key] = self.edge_node_prob[key] / self.edge_node_prob[key].sum()

                if len(self.edge_node_prob[key]) == 0 or cycle_check:
                    del self.edge_node_prob[key]

    def _resolve_node_node_connections(self, removed_node):
        """
        Connects node-to-node transitions in the corresponding dictionary
        for incoming and outgoing transitions from the deleted node
        :param removed_node:str
            The node to be removed
        :return:None
        """
        trigger = 0
        if removed_node in self.node_node_prob[removed_node]:
            self.node_node_prob = self._check_and_remove_node(self.node_node_prob, removed_node, removed_node)
            if removed_node not in self.node_node_prob:
                trigger = 1
        for node in list(self.node_node_prob):
            if removed_node in self.node_node_prob[node]:
                self.node_node_prob[node].drop(removed_node, inplace=True)
                if trigger == 1:
                    self.node_node_prob[node] = self.node_node_prob[node] / self.node_node_prob[node].sum()
                else:
                    self._add_new_node_node_connections(node, removed_node)
        if trigger == 0:
            del self.node_node_prob[removed_node]

    # TODO refactor
    def _resolve_edge_node_connections(self, removed_node):
        """
        Connects edge-to-node transitions in the corresponding dictionary
        for incoming and outgoing transitions from the deleted node.

        If a node with a single incoming transition is deleted during the process,
        a warning about the breaking of the process path will be displayed.

        :param removed_node:str
        :return:None
        """
        if (removed_node, removed_node) in self.edge_node_prob:
            del self.edge_node_prob[(removed_node, removed_node)]
        first_nodes = [
            edge[0] for edge in self.edge_node_prob.keys() if edge[1] == removed_node and edge[0] != removed_node
        ]
        second_nodes = [
            edge[1] for edge in self.edge_node_prob.keys() if edge[0] == removed_node and edge[1] != removed_node
        ]
        for f_node in first_nodes:
            for s_node in second_nodes:
                if (f_node, s_node) in self.edge_node_prob:
                    if f_node == s_node and self._check_for_self_cycles(f_node, removed_node):
                        continue
                    if (
                            removed_node in self.edge_node_prob[(f_node, s_node)]
                            and (s_node, removed_node) not in self.edge_node_prob
                    ):
                        self.edge_node_prob = self._check_and_remove_node(
                            self.edge_node_prob, (f_node, s_node), removed_node
                        )
                else:
                    new_edge = (f_node, s_node)
                    self.edge_node_prob[new_edge] = deepcopy(self.edge_node_prob[(removed_node, s_node)])
                    if f_node == s_node and self._check_for_self_cycles(f_node, removed_node):
                        continue
                    if removed_node in self.edge_node_prob[new_edge]:
                        self.edge_node_prob = self._check_and_remove_node(
                            self.edge_node_prob, new_edge, removed_node
                        )
                    if new_edge in self.edge_node_prob:
                        self._calc_new_transition_duration((f_node, removed_node), (removed_node, s_node))

        for edge in list(self.edge_node_prob):

            if edge[0] != removed_node and edge[1] != removed_node and removed_node in self.edge_node_prob[edge]:
                self.edge_node_prob[edge].drop(removed_node, inplace=True)
                second_edge = (edge[1], removed_node)
                if second_edge in self.edge_node_prob:
                    if removed_node in self.edge_node_prob[second_edge]:
                        self.edge_node_prob = self._check_and_remove_node(
                            self.edge_node_prob, second_edge, removed_node
                        )
                        if second_edge not in self.edge_node_prob:
                            print("Perhaps on of the process flows is broken.")
                            continue
                    self._add_new_edge_node_connections(edge, second_edge, removed_node)
                else:
                    self.edge_node_prob[edge] = self.edge_node_prob[edge] / self.edge_node_prob[edge].sum()
        for edge in list(self.edge_node_prob):
            if edge[0] == removed_node or edge[1] == removed_node:
                del self.edge_node_prob[edge]

    def _add_new_edge_node_connections(self, first_edge, second_edge, removed_node):
        """
        Adds new edge-to-node connections in the appropriate dictionary and
        calculates their probabilities based on known data.

        :param first_edge:tuple of (str,str)
            Edge of type (A, B) -> removed_node (one of the possible transitions)
        :param second_edge:
            Edge of type (B,removed_node)
        :param removed_node:str
            Node to be removed
        :return:None
        """

        for node in self.edge_node_prob[second_edge].index:
            if (first_edge, removed_node) in self.edge_node_node_prob and node in self.edge_node_node_prob[
                first_edge, removed_node
            ]:
                cond_prob = self.edge_node_node_prob[first_edge, removed_node][node]
            else:
                cond_prob = self.edge_node_prob[second_edge][node]
            if node in self.edge_node_prob[first_edge]:
                self.edge_node_prob[first_edge][node] = 1 - (1 - self.edge_node_prob[first_edge][node]) * (
                        1 - cond_prob
                )
            else:
                self.edge_node_prob[first_edge][node] = cond_prob
        self.edge_node_prob[first_edge] = self.edge_node_prob[first_edge] / self.edge_node_prob[first_edge].sum()

    def _check_for_self_cycles(self, node, removed_node):
        """
        Checks self-cycles of type (A, A) -> A
        :param node:str
        :param removed_node:str
        :return:1 if there is self-cycle
                0 otherwise
        """

        if (
                len(self.edge_node_prob[(node, node)]) == 2
                and removed_node in self.edge_node_prob[(node, node)]
                and node in self.edge_node_prob[(node, node)]
        ):
            return 1
        else:
            return 0

    @staticmethod
    def _check_and_remove_node(p_dict, key, node):
        """
        Deletes a transition to a node.
        If after deletion the outgoing node has no transitions, it is also deleted
        :param p_dict:dict
            dict of type node-node or edge-node
        :param key:
            Outgoing node
        :param node:
            Node to be removed
        :return:p_dict:dict
        """

        p_dict[key].drop(node, inplace=True)
        p_dict[key] = p_dict[key] / p_dict[key].sum()

        if len(p_dict[key]) == 0:
            del p_dict[key]

        return p_dict

    def _add_new_node_node_connections(self, first_node, removed_node):
        """
        Adds new node-to-node connections in the appropriate dictionary and
        calculates their probabilities based on known data.

        :param first_node:str
            Node of type A -> removed_node (one of the possible transitions)

        :param removed_node:str
            Node to be removed
        :return:None
        """
        for node in self.node_node_prob[removed_node].index:
            if node == removed_node:
                continue
            if node not in self.edge_node_prob[(first_node, removed_node)]:
                continue

            if node in self.node_node_prob[first_node]:
                self.node_node_prob[first_node][node] = 1 - (1 - self.node_node_prob[first_node][node]) * (
                        1 - self.edge_node_prob[(first_node, removed_node)][node]
                )
            else:
                self.node_node_prob[first_node][node] = self.edge_node_prob[(first_node, removed_node)][node]

        self.node_node_prob[first_node] = self.node_node_prob[first_node] / self.node_node_prob[first_node].sum()

    def mean_duration(self, target="transitions"):
        """
        Calculates a mean duration (execution time) of each node (activity) or edge (transition).

        Parameters
        ----------
        target: {'transitions', 'activities'}, default='transitions'
            Whether to calculate duration of nodes or edges.

        Returns
        -------
        transitions_durations: dict
            Mean duration of each  edge in the process.
        OR
        activities_durations: dict
            Mean duration of each  node in the process.
        """
        transition_duration = {}
        activities_duration = {}
        for key in self._approx_transitions_duration:
            transition_duration[key] = mean(self._approx_transitions_duration[key])
            if key[0] not in activities_duration:
                activities_duration[key[0]] = []
            activities_duration[key[0]].append(transition_duration[key])
        for key in activities_duration:
            activities_duration[key] = mean(activities_duration[key])
        if target == "transitions":
            return transition_duration
        elif target == "activities":
            return activities_duration
        else:
            raise ValueError(f"Expected 'activities' or 'transitions',but got '{target}' instead.")

    def change_activity_duration(self, activity=None, scale=None, threshold=None):
        """
        Changes the mean duration (execution time) of a given activity by setting an upper limit (threshold)
        or times to reduce (scale) it.

        Parameters
        ----------
        activity: str
            Name of the node to change the duration of.

        scale: int, default=None
            Reduces the duration <scale> times.

        threshold: int, default=None
            Upper limit of the duration.
        """
        if threshold:
            old_duration = self.mean_duration("activities")[activity]
            scale = old_duration / threshold

        for key in self._approx_transitions_duration:
            if key[0] == activity:
                self._approx_transitions_duration[key] = self._approx_transitions_duration[key] / scale

        if activity in self._activities_duration:
            self._activities_duration[activity] = self._activities_duration[activity] / scale

    def change_transition_duration(self, transition=None, scale=None, threshold=None):
        """
        Changes the mean duration (execution time) of a given transition by setting an upper limit (threshold)
        or times to reduce (scale) it.

        Parameters
        ----------
        transition: tuple of (str, str)
            Name of the edge to change the duration of.

        scale: int, default=None
            Reduces the duration <scale> times.

        threshold: int, default=None
            Upper limit of the duration.
        """
        if threshold:
            old_duration = self.mean_duration("transitions")[transition]
            scale = old_duration / threshold

        self._approx_transitions_duration[transition] = self._approx_transitions_duration[transition] / scale

    def to_initial_state(self):
        """
        Returns the process generated by the simulation model to its initial state.
        """

        self.node_node_prob = deepcopy(self._reserved_node_node_prob)
        self.edge_node_prob = deepcopy(self._reserved_edge_node_prob)
        self.edge_node_node_prob = deepcopy(self._reserved_edge_node_node_prob)
        self._activities_duration = deepcopy(self._reserved_activities_durations)
        if self._reserved_transitions_durations is not None:
            self._approx_transitions_duration = deepcopy(self._reserved_transitions_durations)
        else:
            raise RuntimeWarning("You need to run the simulation first")
        self.starting_acts_prob = deepcopy(self._reserved_starting_acts_prob)

    def get_sequential_nodes(self):
        """
        Looks for nodes of the following type: A-> B-> C,
        where exactly one incoming transition is included in B.
        It is assumed that then B and C can be executed in parallel.
        :return:sequential_nodes:list of lists
            Required combinations of nodes
        """
        sequential_nodes = []
        possible_nodes = unique(list(chain(*[self.node_node_prob[node].index for node in self.node_node_prob])))
        for node in self.node_node_prob:
            if node != "end" and node in possible_nodes:
                nodes_list = self._check_if_serially_connected(node, self.node_node_prob, 0)
                if nodes_list is not None and nodes_list not in sequential_nodes:
                    sequential_nodes.append(nodes_list)
                else:
                    continue
        return sequential_nodes

    def _check_if_serially_connected(self, node, probs, length, nodes=None):
        """
        Recursively defines all nodes that match the above requirements
        :param node:str
        :param probs:Series
        :param length:int
        :param nodes:list
        :return:nodes:list
            Serially connected nodes

        """

        if length == 0:
            nodes = [node]
        if length >= 2:
            return nodes
        if len(probs[node]) == 1 or (len(probs[node]) == 2 and node in probs[node].index) or length == 0:
            target_node = [x for x in probs[node].index if x != node][0]
            if target_node == "end":
                return None

            nodes.append(target_node)
            length += 1

            return self._check_if_serially_connected(target_node, probs, length, nodes)
        else:
            return None

    def parallelize_nodes(self, nodes):
        """
        Parallels nodes B and C in a chain of the form A-> B-> C.
        After that, a transition can be made from A to B and C:A->B and A->C.
        :param nodes:list
            Nodes to be parallelized
        :return:None
        """
        self.node_node_prob[nodes[0]] = self.node_node_prob[nodes[0]].append(self.node_node_prob[nodes[1]])
        self.node_node_prob[nodes[0]] = self.node_node_prob[nodes[0]] / self.node_node_prob[nodes[0]].sum()
        self.node_node_prob[nodes[1]] = self.node_node_prob[nodes[2]]
        for key in list(self.edge_node_prob.keys()):
            if key[1] == nodes[0]:
                self.edge_node_prob[key] = self.edge_node_prob[key].append(
                    self.edge_node_prob[(nodes[0], nodes[1])]
                )
                self.edge_node_prob[key] = self.edge_node_prob[key] / self.edge_node_prob[key].sum()

        self.edge_node_prob[(nodes[0], nodes[1])] = self.edge_node_prob[(nodes[1], nodes[2])]
        self._transitions_duration[(nodes[0], nodes[2])] = self._transitions_duration[(nodes[1], nodes[2])]
        self.edge_node_prob[(nodes[0], nodes[2])] = self.edge_node_prob.pop((nodes[1], nodes[2]))
        for key in list(self.edge_node_prob):
            if key[0] == nodes[2]:
                self.edge_node_prob[(nodes[1], key[1])] = self.edge_node_prob[key]
                self._transitions_duration[(nodes[1], key[1])] = self._transitions_duration[key]

    @staticmethod
    def _get_starting__activities_prob(data_holder):
        """
        Calculates the probabilities for activities to be a start activity in a trace.

        Parameters
        ----------
        data_holder: sberpm.DataHolder

        Returns
        -------
        probs: Series
            Index: activity_name, value: probability.
        """
        id_mask = data_holder.data[data_holder.id_column] != data_holder.data[data_holder.id_column].shift(1)
        activities = data_holder.data[data_holder.activity_column][id_mask]
        return activities.value_counts(normalize=True)

    def _get_node_node_cond_prob(self, data_holder):
        """
        Gets the conditional probabilities of the second nodes for each first node in a pair (edge).

        Parameters
        ----------
        data_holder: sberpm.DataHolder

        Returns
        -------
        probs: dict of {str, Series}
            Key: first node, value: pandas.Series: Index: second node, Value: its conditional probability.
        """
        df = DataFrame(
            {
                "node_1": self._supp_data[data_holder.activity_column],
                "node_2": self._supp_data[data_holder.activity_column].shift(-1),
            }
        )
        id_mask = self._supp_data[data_holder.id_column] == self._supp_data[data_holder.id_column].shift(-1)
        df = df[id_mask]

        # Series, Multiindex: (node_1, node_2), value: node_2_prob for each known node_1 (=conditional probability).
        multi_probs = df.groupby("node_1")["node_2"].value_counts(normalize=True)
        return self._to_prob(multi_probs)

    def _get_edge_node_cond_prob(self, data_holder):
        """
        Gets the conditional probabilities of the third nodes for each pair of first two nodes.

        Parameters
        ----------
        data_holder: sberpm.DataHolder

        Returns
        -------
        probs: dict of {(str, str), Series}
            Key: first two nodes, value: pandas.Series: Index: third node, Value: its conditional probability.
        """
        df = DataFrame(
            {
                "nodes_12": zip(
                    self._supp_data[data_holder.activity_column],
                    self._supp_data[data_holder.activity_column].shift(-1),
                ),
                "node_3": self._supp_data[data_holder.activity_column].shift(-2),
            }
        )
        id_mask_1 = self._supp_data[data_holder.id_column] == self._supp_data[data_holder.id_column].shift(-1)
        id_mask_2 = self._supp_data[data_holder.id_column] == self._supp_data[data_holder.id_column].shift(-2)
        df = df[id_mask_1 & id_mask_2]

        # Series, Multiindex: ((node_1, node_2), node_3), value: node_3_prob for each (node_1, node_2) pair.
        multi_probs = df.groupby("nodes_12")["node_3"].value_counts(normalize=True)
        return self._to_prob(multi_probs)

    def _get_edge_node_node_cond_prob(self, data_holder):
        """
        Gets the conditional probabilities of the fourth nodes for each pair of first three nodes.

        Parameters
        ----------
        data_holder: sberpm.DataHolder

        Returns
        -------
        probs: dict of {(str, str), Series}
            Key: first two nodes, value: pandas.Series: Index: third node, Value: its conditional probability.
        """
        df = DataFrame(
            {
                "nodes_123": zip(
                    zip(
                        self._supp_data[data_holder.activity_column],
                        self._supp_data[data_holder.activity_column].shift(-1),
                    ),
                    self._supp_data[data_holder.activity_column].shift(-2),
                ),
                "node_4": self._supp_data[data_holder.activity_column].shift(-3),
            }
        )
        id_mask_1 = self._supp_data[data_holder.id_column] == self._supp_data[data_holder.id_column].shift(-1)
        id_mask_2 = self._supp_data[data_holder.id_column] == self._supp_data[data_holder.id_column].shift(-2)
        id_mask_3 = self._supp_data[data_holder.id_column] == self._supp_data[data_holder.id_column].shift(-3)
        df = df[id_mask_1 & id_mask_2 & id_mask_3]

        # Series, Multiindex: ((node_1, node_2), node_3), value: node_3_prob for each (node_1, node_2) pair.
        multi_probs = df.groupby("nodes_123")["node_4"].value_counts(normalize=True)
        return self._to_prob(multi_probs)

    @staticmethod
    def _to_prob(multiindex_prob_series):
        """
        Converts multiindex Series to dict of conditional probabilities.

        Parameters
        ----------
        multiindex_prob_series: pandas.Series
            Multiindex: (obj_1, obj_2), values: conditional probability of object_2 for given object_1.

        Returns
        -------
        second_object_cond_probs: dict of (obj_1, pandas.Series)
            Key: first object (node: str, or edge: tuple(str, str)).
            Value: Series, Index: object_2 (node: str), values: conditional probabilities: float.
        """
        second_object_cond_probs = {}
        # obj2_probs: part of multiindex_prob_series but with the same obj_1 in the index (obj_1, obj_2).
        for obj1, obj2_probs in multiindex_prob_series.groupby(level=0):
            obj2_probs = obj2_probs.droplevel(0)  # Series, Index: obj_2, value: obj_2_prob.
            clean_probs = obj2_probs / obj2_probs.sum()
            clean_probs = clean_probs[clean_probs > 0.05]
            clean_probs = clean_probs / clean_probs.sum()
            second_object_cond_probs[obj1] = clean_probs.rename()

        return second_object_cond_probs

    def group(self, by):
        """
        Groups the generated data by either id, unique num_of_traces or activities and calculates time metrics.

        Parameters
        ----------
        by: {'Id', 'Trace', 'Activity'}
            Type of grouping the data.

        Returns
        -------
        df: DataFrame
            Data grouped by id, num_of_traces or activities with corresponding time metrics.
        """

        if by == "Id":
            return (
                self.generated_log.groupby(self.holder.id_column)
                    .agg({self.holder.activity_column: [tuple, len], self.holder.time_work_column: "sum"})
                    .reset_index()
            )
        elif by == "Trace":
            log_gen_group = (
                self.generated_log.groupby(self.holder.id_column)
                    .agg({self.holder.activity_column: tuple, self.holder.time_work_column: "sum"})
                    .reset_index()
            )
            return (
                log_gen_group.groupby(self.holder.activity_column)[self.holder.time_work_column]
                    .agg(["count", "sum", "min", "max", "median", "mean", "std"])
                    .reset_index()
            )
        elif by == "Activity":
            return (
                self.generated_log.groupby(self.holder.activity_column)[self.holder.time_work_column]
                    .agg(["count", "sum", "min", "max", "median", "mean", "std"])
                    .reset_index()
            )
